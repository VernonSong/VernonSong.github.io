---
layout: post
title: 机器学习学习笔记【8】
subtitle: 聚类 降维
date: 2017-12-13 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml8.jpg"
catalog: true
tags:
    - 机器学习
---

### K-均值算法

#### 算法简介
K-均值算法时最普及的聚类算法，算法接受一个未标记的数据集，然后将数据集聚类成不同的组。其思想是：

1. 选择K个随机的点，称为**聚类中心**
2. 对数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类
3. 计算每一组的平均值，将改组所关联的中心点移动到平均值的位置
4. 重复2-4过程直至中心点不再变化

对于K-均值算法，我们的目标就是最小化所有数据点与其所关联的聚类中心点之间的距离和，因此其代价函数为
而我们在每次迭代时，都在减少这个代价，直到中心点不在变化，停止迭代。

#### 随机初始化
在运行K-均值算法之前，我们要随机初始化所有聚类中心，一般随机选择K个训练实例作为初始聚类中心。但为了避免停留在局部极小值处，通常需要多次运行K-均值算法，每次都重新随机初始化，最后比较每次K-均值结果，选择代价函数最小的结果。这种方法在K比较小时是可行的，但当K比较大时并不会明显的改善结果。

#### 选择聚类数
对于如何选择聚类数目，有一个方法叫做“肘部法则”。其意思是，当我们不断改变K的值，其代价函数与K的关系呈现下图这样的关系时，我们可以把那个在形如肘关节位置的K作为正确K值。但更多时候，我们会选择根据需求选择聚类数目
![](https://github.com/VernonSong/Storage/blob/master/image/ML10.png?raw=true)
### 降维
降维是指在某些限定条件下，降低随机变量个数，得到一组“不相关”主变量的过程。 降维可进一步细分为特征选择和特征提取两大方法。

特征选择假定数据中包含大量冗余或无关变量（或称特征、属性、指标等），旨在从原有变量中找出主要变量。

特征提取是将高维数据转化为低维数据的过程。在此过程中可能舍弃原有数据、创造新的变量，其代表方法为主成分分析。

#### 主成分分析

主成分分析（PCA）是最常见的降维算法。在PCA中，我们要做的是找到一个方向向量，当我们把所有的数据都投射到该向量时，我们希望投射平均方差尽可能小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。

与逻辑回归不同，主成分分析最小化的是投影误差，而线性回归尝试的是最小化预测误差。
![](https://github.com/VernonSong/Storage/blob/master/image/ML9.png?raw=true)
