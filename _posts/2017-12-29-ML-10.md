---
layout: post
title: 机器学习学习笔记【10】
subtitle:   大规模数据下的机器学习
date: 2017-12-28 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml10.jpg"
catalog: true
tags:
    - 机器学习
---

### 随机梯度下降
由于我们要在每一次梯度下降迭代时计算训练集的误差平方和，当训练的数据集非常大的时候，计算量也会随之变得巨大，这时可以尝试使用随机梯度下降法代替批量梯度下降法。

在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价，在对训练集打乱顺序后，在每次计算后就进行梯度下降，更新参数θ。因此，在一次迭代没有完成时，随机梯度下降算法便已经走出了很远。但这样并不是每一步都朝着“正确”的方向前进，因此算法虽然走向全局最小值的位置，但可能无法站到最小值那个点，而只是在附近徘徊，不过对模型来说已经足够。

### 小批量梯度下降
小批量算法介于批量梯度下降算法与随机梯度下降算法之间，每计算常数b次训练实例，便更新一次参数θ。通常我们会令b在2-200之间。这样可以用向量化方式来循环b个训练实例。

### 在线学习
许多大型网络公司会使用在线学习（Online Learning）算法，从数据流而非静态的数据集中学习，这样做的好处是，不需要存储一个庞大的数据集，并且算法能不断的适应用户。之前从全部的静态数据集中学习的方法被称为批量学习（Batch Learning）。

### 映射化简
映射化简（MapReduce）可以说主要是从硬件上加速大规模数据下机器学习的性能，即将计算分配给个计算机，让每个计算机处理数据集的一个子集，然后汇总计算结果处理。