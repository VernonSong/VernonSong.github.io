---
layout: post
title:  ResNet原理与实现
subtitle: 
date: 2018-06-27 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-resnet.jpg"
catalog: true
mathjax: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
虽然深层卷积神经网络在很多计算机视觉任务取得了不错的效果，但随着网络加深，网络会越来越难以训练。而通过加入残差模块，让梯度流能直接传到前面的层，这样即使网络比较深依然能快速收敛。

## 原理
### 深层神经网络问题
在深层神经网络中，越深的层将学习越高级的特征，理论上拥有更多层数的网络效果应该更好。但实践中，并非越深的网络效果越好，这主要有两方面原因，其一随着网络加深，越容易发生梯度消失与梯度爆炸，此问题通过Batch Normalization等方法可以在很大程度上解决，但依然是深层神经网络的标志性问题。其二是退化，实验表明，过于深的网络性能产生退化，这种退化不是更强的表达能力导致过拟合，而是即使在训练集上也难以获得与更浅的网络一致的准确率。


### 残差结构
假设一个较浅的$l$层网络$x_l$已经有不错的准确率，在该网络之后再加入若干层，组合成$L$层的新网络$x_L$，起码误差不应该增加，但由于实际中会出现退化现象，为保证网络有之前的精度，使用残差的思想设计网络结构：

$$
x_{l+1}=x_l+\mathcal {F}(x_l,\mathcal {W_l})
$$

在这样的结构中，$f(x_l,W_l)$可以看作目标网络$x_{l+1}$与$x_l$的差，当$f(x_l,W_l)=0$时，网络等同于浅层网络$x_l$。通过明确的使用$f(x_l,W_l)$来拟合$x_{l+1}$与$x_l$的差，而不是直接使用它去拟合目标网络$x_{l+1}$，使残差模块$f(x_l,W_l)$学到的信息全部都是额外信息，不会破坏使浅层网络$x_l$中已经学到的信息，因此网络会更容易优化。

残差结构通过shortcut实现

![img](/img/in-post/post-resnet/post-resnet1.png)

### 反向传播
拥有多个残差模块的网络可以表示为：

$$
x_L=x_l+\sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})
$$

相比于线性堆彻的网络，这样的结构有更好的反向传播特性

$$
\frac{\partial E}{\partial x_l} = \frac{\partial E}{\partial x_L} \frac{\partial  x_L}{\partial x_l} = \frac{\partial E}{\partial x_L}(1+\frac{\partial \sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})}{\partial x_l})
$$

通过shortcut，梯度$\frac{\partial E}{\partial x_l}$被分为两部分，其中$ \frac{\partial E}{\partial x_L}$不经过任何权重层，保证了信息一定能传递到浅层$l$，同时只要$(1+\frac{\partial \sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})}{\partial x_l})$不为0，梯度便不会消失。

### shortcut支路

除上文中简洁的shortcut之外，作者还设计了其他shortcut方案
![img](/img/in-post/post-resnet/post-resnet2.png)

但实验结果表明还是最原始的shortcut方案效果最好，给shortcut进行缩放，门控，1×1卷积以及dropout均会使网络性能变差，因此残差单元关键在于让shortcut中信息以其原本的状态传递。

### residual支路
对于残差支路，作者同样设计了多种不同方案
![img](/img/in-post/post-resnet/post-resnet3.png)
![img](/img/in-post/post-resnet/post-resnet4.png)
当把BN层放在addition层后面后，网络效果显著降低，原因在于这样的设计阻碍了shortcut原始信息的传递。而如果把ReLu层作为残差支路的最后一层，支路的结果将会一直是非负值。其余三种设计差距较小，由于BN->ReLU->weight在信心传入残差
## 实现




