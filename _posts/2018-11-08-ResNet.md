---
layout: post
title:  ResNet原理与实现
subtitle:
date: 2018-06-27 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-resnet.jpg"
catalog: true
mathjax: true
tags:
- 深度学习
- 论文翻译
---

## 概述

虽然深层卷积神经网络在很多计算机视觉任务取得了不错的效果，但随着网络加深，网络会越来越难以训练。而通过加入残差模块，让梯度流能直接传到前面的层，这样即使网络比较深依然能快速收敛。

原理

深层神经网络问题

在深层神经网络中，越深的层将学习越高级的特征，理论上拥有更多层数的网络效果应该更好。但实践中，并非越深的网络效果越好，这主要有两方面原因，其一随着网络加深，越容易发生梯度消失与梯度爆炸，此问题通过Batch Normalization等方法可以在很大程度上解决，但依然是深层神经网络的标志性问题。其二是退化，实验表明，过于深的网络性能产生退化，这种退化不是更强的表达能力导致过拟合，而是即使在训练集上也难以获得与更浅的网络一致的准确率。

残差模块

假设一个较浅的网络$x$已经有不错的准确率，在该网络之后再加入若干层，起码误差不应该增加，但由于实际中会出现退化现象，为保证网络有之前的精度，将$x$作为目标网络，添加的层为$f(x)，有：

$$
h(x)=f(x)+x
$$

通过明确的使用$f(x)$来拟合目标网络$h(x)$与$x$的差，而不是直接使用一个线性堆叠的网络，使网络更容易优化，这种结构称为残差映射。这样当$f(x)=0$时，网络等同于浅层网络$x$。

残差模块通过shortcut实现

file:///Users/songweinan/Library/Containers/com.bloombuilt.dayone-mac/Data/dayone-renderable-code-block:/B00634DF-8D0A-460D-9A4E-B447380D1D7E

相比于线性堆彻的网络，这样的结构有更好的反向传播特性

$$
\frac{\partial E}{\partial x} = \frac{\partial E}{\partial h(x)} \frac{\partial  h(x)}{\partial x} = \frac{\partial E}{\partial h(x)}(1+\frac{\partial h(x)}{\partial x})
$$

通过shortcut，梯度被分为两部分，其中一部分可以直接传递到$x$，减少了梯度消失的风险。

实现