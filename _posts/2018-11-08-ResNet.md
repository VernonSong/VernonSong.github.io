---
layout: post
title:  ResNet原理与实现
subtitle: 
date: 2018-06-27 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-resnet.jpg"
catalog: true
mathjax: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
虽然深层卷积神经网络在很多计算机视觉任务取得了不错的效果，但随着网络加深，网络会越来越难以训练。而通过加入残差模块，让梯度流能直接传到前面的层，这样即使网络比较深依然能快速收敛。

## 原理
### 深层神经网络问题
在深层神经网络中，越深的层将学习越高级的特征，理论上拥有更多层数的网络效果应该更好。但实践中，并非越深的网络效果越好，这主要有两方面原因，其一随着网络加深，越容易发生梯度消失与梯度爆炸，此问题通过Batch Normalization等方法可以在很大程度上解决，但依然是深层神经网络的标志性问题。其二是退化，实验表明，过于深的网络性能产生退化，这种退化不是更强的表达能力导致过拟合，而是即使在训练集上也难以获得与更浅的网络一致的准确率。


### 残差结构
假设一个较浅的$l$层网络$x_l$已经有不错的准确率，在该网络之后再加入若干层，组合成$L$层的新网络$x_L$，起码误差不应该增加，但由于实际中会出现退化现象，为保证网络有之前的精度，使用残差的思想设计网络结构：

$$
x_{l+1}=x_l+\mathcal {F}(x_l,\mathcal {W_l})
$$

在这样的结构中，$f(x_l,W_l)$可以看作目标网络$x_{l+1}$与$x_l$的差，当$f(x_l,W_l)=0$时，网络等同于浅层网络$x_l$。通过明确的使用$f(x_l,W_l)$来拟合$x_{l+1}$与$x_l$的差，而不是直接使用它去拟合目标网络$x_{l+1}$，使残差模块$f(x_l,W_l)$学到的信息全部都是额外信息，不会破坏使浅层网络$x_l$中已经学到的信息，因此网络会更容易优化。

残差结构通过shortcut实现

![img](/img/in-post/post-resnet/post-resnet1.png)

### 反向传播
拥有多个残差模块的网络可以表示为：

$$
x_L=x_l+\sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})
$$

相比于线性堆彻的网络，这样的结构有更好的反向传播特性

$$
\frac{\partial E}{\partial x_l} = \frac{\partial E}{\partial x_L} \frac{\partial  x_L}{\partial x_l} = \frac{\partial E}{\partial x_L}(1+\frac{\partial \sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})}{\partial x_l})
$$

通过shortcut，梯度$\frac{\partial E}{\partial x_l}$被分为两部分，其中$ \frac{\partial E}{\partial x_L}$不经过任何权重层，保证了信息一定能传递到浅层$l$，同时只要$(1+\frac{\partial \sum_{i=1}^{L-1}\mathcal {F}(x_l,\mathcal {W_l})}{\partial x_l})$不为0，梯度便不会消失。

### shortcut支路

除上文中简洁的shortcut之外，作者还设计了其他shortcut方案
![img](/img/in-post/post-resnet/post-resnet2.png)

但实验结果表明还是最原始的shortcut方案效果最好，给shortcut进行缩放，门控，1×1卷积以及dropout均会使网络性能变差，因此残差单元关键在于让shortcut中信息以其原本的状态传递。

### residual支路
对于残差支路，作者同样设计了多种不同方案
![img](/img/in-post/post-resnet/post-resnet3.png)
![img](/img/in-post/post-resnet/post-resnet4.png)
当把BN层放在addition层后面后，网络效果显著降低，原因在于这样的设计阻碍了shortcut原始信息的传递。而如果把ReLU层作为残差支路的最后一层，支路的结果将会一直是非负值，残差部分的表达能力会大打折扣。

其余三种设计差距较小，原始的设计采用的是Post-activation，将会使下一个残差单元的两条之路均受到激活函数$x_{l+1}=f(y_l)$影响：

$$
y_{l+1}=f(y_l)+\mathcal {F}(f(y_l),\mathcal {W_l})
$$

而如果采用非对称的方式，让激活函数$\hat{f}$只影响残差支路$\mathcal{F}$，下一个残差单元将会是：

$$
y_{l+1}=y_l+\mathcal {F}(\hat{f}(x_l),\mathcal {W_l})
$$

可以发现非对称的设计能让下一个残差单元的shortcut支路保持其原有状态，更符合残差结构设计理念。而采用非对称设计后，$\hat{f}$将会作为下一个残差单元的pre-activation。

![img](/img/in-post/post-resnet/post-resnet5.png)

对比ReLU-only pre-activation和full pre-activation两种方案，由于full pre-activation中信息在进入残差模块时先进行了BatchNormalization，准确率更高一些，ResNet v2使用的就是这种设计。

## 实现

### Tensorflow

```python
def subsample(inputs, factor, scope=None):
  """Subsamples the input along the spatial dimensions.
  Args:
    inputs: A `Tensor` of size [batch, height_in, width_in, channels].
    factor: The subsampling factor.
    scope: Optional variable_scope.
  Returns:
    output: A `Tensor` of size [batch, height_out, width_out, channels] with the
      input, either intact (if factor == 1) or subsampled (if factor > 1).
  """
  if factor == 1:
    return inputs
  else:
    return slim.max_pool2d(inputs, [1, 1], stride=factor, scope=scope)
  

def conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):
  """Strided 2-D convolution with 'SAME' padding.
  When stride > 1, then we do explicit zero-padding, followed by conv2d with
  'VALID' padding.
  Note that
     net = conv2d_same(inputs, num_outputs, 3, stride=stride)
  is equivalent to
     net = slim.conv2d(inputs, num_outputs, 3, stride=1, padding='SAME')
     net = subsample(net, factor=stride)
  whereas
     net = slim.conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME')
  is different when the input's height or width is even, which is why we add the
  current function. For more details, see ResnetUtilsTest.testConv2DSameEven().
  Args:
    inputs: A 4-D tensor of size [batch, height_in, width_in, channels].
    num_outputs: An integer, the number of output filters.
    kernel_size: An int with the kernel_size of the filters.
    stride: An integer, the output stride.
    rate: An integer, rate for atrous convolution.
    scope: Scope.
  Returns:
    output: A 4-D tensor of size [batch, height_out, width_out, channels] with
      the convolution output.
  """
  if stride == 1:
    return slim.conv2d(inputs, num_outputs, kernel_size, stride=1, rate=rate,
                       padding='SAME', scope=scope)
  else:
    kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)
    pad_total = kernel_size_effective - 1
    pad_beg = pad_total // 2
    pad_end = pad_total - pad_beg
    inputs = tf.pad(inputs,
                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])
    return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,
                       rate=rate, padding='VALID', scope=scope)


@slim.add_arg_scope
def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,
               outputs_collections=None, scope=None):
  """Bottleneck residual unit variant with BN before convolutions.
  This is the full preactivation residual unit variant proposed in [2]. See
  Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck
  variant which has an extra bottleneck layer.
  When putting together two consecutive ResNet blocks that use this unit, one
  should use stride = 2 in the last unit of the first block.
  Args:
    inputs: A tensor of size [batch, height, width, channels].
    depth: The depth of the ResNet unit output.
    depth_bottleneck: The depth of the bottleneck layers.
    stride: The ResNet unit's stride. Determines the amount of downsampling of
      the units output compared to its input.
    rate: An integer, rate for atrous convolution.
    outputs_collections: Collection to add the ResNet unit output.
    scope: Optional variable_scope.
  Returns:
    The ResNet unit's output.
  """
  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:
    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
    preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')
    if depth == depth_in:
      shortcut = subsample(inputs, stride, 'shortcut')
    else:
      shortcut = slim.conv2d(preact, depth, [1, 1], stride=stride,
                             normalizer_fn=None, activation_fn=None,
                             scope='shortcut')

    residual = slim.conv2d(preact, depth_bottleneck, [1, 1], stride=1,
                           scope='conv1')
    residual = conv2d_same(residual, depth_bottleneck, 3, stride,
                                        rate=rate, scope='conv2')
    residual = slim.conv2d(residual, depth, [1, 1], stride=1,
                           normalizer_fn=None, activation_fn=None,
                           scope='conv3')

    output = shortcut + residual

    return slim.utils.collect_named_outputs(outputs_collections,
                                            sc.name,
                                            output)

```

## 参考
> [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)
> <br/>
> [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)
> <br/>
> [https://github.com/tensorflow/models/blob/master/research/slim/nets/resnet_v2.py](https://github.com/tensorflow/models/blob/master/research/slim/nets/resnet_v2.py)



