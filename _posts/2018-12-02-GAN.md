---
layout: post
title:  GAN原理与实现
subtitle: 
date: 2018-11-22 00:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-gan.jpg"
catalog: true
mathjax: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
由于深度神经网络在使用最大似然估计等算法时遇到了难以解决的概率计算问题，因此在计算机视觉领域取得成绩的基本都是判别模型，而生成对抗网络（Generative Adversarial Nets）的出现改变了这一现象。该网络受启发于二人零和博弈（two-player game），通过一个生成模型和一个判别模型互相博弈，巧妙的解决了生成模型的优化问题。

## 原理
### GAN思想
生成对抗网络中，生成模型的任务是学习数据$x$的分布$p_g$，其输入为随机噪声$z$，                                                                                                                                                                                                                                              为了让生成模型获得足够的表达能力，用神经网络$G(z)$构成生成模型$G$。而判别模型$D$同样由一个神经网络构成，它的任务是判断输入$x$是真实数据分布$p_{data}$还是生成模型产生的分布$p_g$，生成模型$G$的目标是最小化$log(1-D(G(z)))$。这样便把$D$与$G$的训练转换成了关于函数$V(G,D)$的极小化与极大化的二人零和博弈问题。

$$
\mathop{\mathrm{min}}\limits_{G}\mathop{\mathrm{max}} \limits_{D} V(D,G) =\mathbb{E}_{x\thicksim p_{data}(x)}[\mathrm{log} D(x)]+\mathbb{E}_{z \thicksim p_z(z)}[\mathrm{log}(1-D(G(z)))] 
$$

用一个例子来比喻就是：生成模型$G$是一个假币制造者团伙，其目的是生成出与真币难以区分的假币；而判别模型$D$是试图区分出真币和假币的警察。在竞争的过程中，双方都不断提升自己的方法，最终导致假币制造团伙的假币足以以假乱真。

### GAN推导

对于给定$G$，最大化$V(D,G)$以获得最优$D$：

$$
\begin{align*}
V(G,D)&=\int_x p_{data}(x) \mathrm{log}(D(x))\mathrm{d}x+\int_z p_z(z) \mathrm{log}(1-D(G(z))\mathrm{d} z
\newline &=\int_x p_{data}(x) \mathrm{log}(D(x))+ p_g(x)\mathrm{log}(1-D(x)\mathrm{d} x
\end{align*}
$$

由于$G$已给定，因此$p_{data}(x)$和$p_g(x)$均为常量

$$
f(y)=a \mathrm{log}(y)+b \mathrm{log}(1-y)
$$

求函数一阶导数以得到其极值点：

$$
f'(y)=0 \Rightarrow \frac{a}{y}-\frac{b}{1-y}=0  \Rightarrow y= \frac{a}{a+b}
$$

当$y=\frac{a}{a+b}$时，函数二阶导小于0，此时函数取得极大值。因此，GAN的损失可以转化为：

$$
\begin{align*}
C(G) &=  \max \limits_{D}(G,D)
\newline &=\mathbb{E}_{x \thicksim p_{data}}\left [\mathrm{log}\frac{p_{data}(x)}{p_{data}(x)+p_g(x))}\right ]+\mathbb{E}_{x \thicksim p_g}\left [1-\mathrm{log}\frac{p_g(x)}{p_{data}(x)+p_g(x))}\right ]
\end{align*}
$$

当$p_{data}(x)=p_g(x)$时，有：

$$
C(G)=\int_x p_{data}(x) \mathrm{log}\frac{1}{2}+ p_g(x)\mathrm{log}(1-\frac{1}{2})\mathrm{d} x
=- \mathrm{log}4
$$

下面证明$-\mathrm{log}4$为函数最小值：

$$
\begin{align*}
C(G) =&\int_x (\mathrm{log2 -log2})p_{data}(x)+p_{data}(x) \mathrm{log}\left (\frac{p_{data}(x)}{p_g(x)+p_{data}(x)} \right ) 
\newline & + (\mathrm{log2 -log2})p_g(x)+p_{data}(x) \mathrm{log}\left (\frac{p_g(x)}{p_g(x)+p_{data}(x)} \right ) \mathrm{d}x
\newline =&-\mathrm{log}2 \int_x p_g(x)+p_{data}(x) \mathrm{d}x
\newline &+ \int_x p_{data}(x) \left( \mathrm{log}2 +\mathrm{log} \left ( \frac{p_{data}(x)}{p_g(x)+p_{data}(x)} \right )\right )
\newline &+ p_g(x) \left ( \mathrm{log}2+\mathrm{log} \left ( \frac{p_g(x)}{p_g(x)+p_{data}(x)} \right ) \right )  \mathrm{d}x
\newline = & -\mathrm{log}4+\left( \mathrm{log}2+\mathrm{log}\left( \frac{p_{data}(x)}{p_g(x)+p_{data}(x)} \right)\right)
\newline &+\left( \mathrm{log}2+\mathrm{log}\left(\frac{p_g(x)}{p_g(x)+p_{data}(x)} \right ) \right )
\newline =& -\mathrm{log4}+\int_x p_{data}(x) \frac{p_{data}(x)}{(p_g(x)+p_{data}(x))/2} \mathrm{d}x
\newline &+ \int_x p_g(x)\frac{p_g(x)}{(p_g(x)+p_{data}(x))/2} \mathrm{d}x
\end{align*}
$$

因为后两项正好是[KL散度](https://zh.wikipedia.org/wiki/%E7%9B%B8%E5%AF%B9%E7%86%B5)，而KL散度总是非负，所以$-\mathrm{log}4$为$C(G)$最小值。而$p_{data}(x)=p_g(x)$时函数值为$-\mathrm{log}4$，则有函数在$p_{data}(x)=p_g(x)$时取得最小值。

### GAN训练
GAN的训练过程可以用下图表示
![](/img/in-post/post-gan.png)
其中黑色虚线为真实数据分布，蓝色虚线为判别器，绿色实线为生成数据分布，黑色箭头为生成器。训练时将会交替训练判别器与生成器，通过判别器的梯度来引导生成器产生的数据接近真实数据分布。

形式化的表示即为：
- - - - -
**for** number of training iterations **do**
<br/>    &emsp; **for** $k$ steps **do** 
- 在噪声先验分布为$p_g(z)$的m个噪声样本$\{z^{(1)},\dots ,z^{(m)} \}$中取一个minibatch
- 在数据分布为$p_g(z)$的m个训练样本$\{x^{(1)},\dots ,x^{(m)} \}$中取一个minibatch
- 通过梯度上升算法更新判别器

$$
\Delta_{\theta_d} \frac{1}{m}\sum_{i=1}^{m} \left [ \mathrm{log}D (x^{(i)})+\mathrm{log}(1-D(G(z^{(i)}))  \right ]
$$

<br/>    &emsp; **endfor**
- 在噪声先验分布为$p_g(z)$的m个噪声样本$\{z^{(1)},\dots ,z^{(m)} \}$中取一个minibatch
- 通过梯度下降算法更新生成器

$$
\Delta_{\theta_g} \frac{1}{m}\sum_{i=1}^{m} \mathrm{log}(1-D(G(z^{(i)})) 
$$

**endfor**

- - - - -

### cGANs
Conditional Generative Adversarial Nets是对GAN的扩展，普通的GAN建模时没有约束，生成出的图片不可控，因此cGAN通过加入给生成器和判别器增加条件变量$y$，来构造带有条件约束的生成对抗网络：

$$
\mathop{\mathrm{min}}\limits_{G}\mathop{\mathrm{max}} \limits_{D} V(D,G) =\mathbb{E}_{x\thicksim p_{data}(x \mid y)}[\mathrm{log} D(x)]+\mathbb{E}_{z \thicksim p_z(z)}[\mathrm{log}(1-D(G(z \mid y)))] 
$$

这个改进虽然简单，但更有效的发挥了GAN在数据生成上的效果。因此，之后的很多GAN的变种和应用都是基于cGAN。

## 实现
### Tensorflow API
Tensorflow含有GAN API[tf.contrib.gan](https://www.tensorflow.org/api_docs/python/tf/contrib/gan)来方便GAN网络的训练，其教程[TFGAN Tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb)包含了基于mnist数据集的手写数字生成，写的非常详细。

## 参考
> [Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661.pdf)
> <br/>
> [Conditional Generative Adversarial Nets](https://arxiv.org/pdf/1411.1784.pdf)
>  <br/>
> [机器之心GitHub项目：GAN完整理论推导与实现，Perfect！](https://www.jiqizhixin.com/articles/2017-10-1-1)
> 