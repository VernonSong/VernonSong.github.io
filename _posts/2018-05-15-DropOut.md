---
layout: post
title:  dropout原理与实现
subtitle: 
date: 2018-04-27 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-dropout.jpg"
catalog: true
mathjax: true
tags:
    - 深度学习    
    - 论文翻译
---

## 概述

深层神经网络通过大量的非线性的隐藏层来使它们的模型有强大的表达力，网络能在输入与输出之间学到很复杂的关系。然而，由于训练集有限，许多复杂的关系其实只是样本的噪声，它们仅仅存在于训练集，而不在测试集中，这便是导致过拟合的原因，目前有许多方法减少过拟合，dropout便是其中之一，此方法通过随机舍弃隐藏层中的一部分结点，达到减少过拟合的效果。

## 原理
dropout受启发于大自然，自然界中存在有性繁殖和无性繁殖两种繁殖方式，看起来无性繁殖要更有优势，因为优秀的基因可以直接传给后代，而有性繁殖则像是把优秀的基因拆散，导致后代不一定能够继承父母的优秀基因。但高等动物却都是有性繁殖，这是为什么？

有一个观点是，自然选择并不是看族群中个别基因的能力，而是看基因混合后的能力，这样每个基因都要去变得有用，而不是依赖那一小部分优秀的基因。

同理，对于神经网络，我们把每个神经元看做一个基因，通过dropout，我们让神经元随机组合在一起，这样每个神经元都要去学着去生成有用的特征，而不是依赖其他能力较强的神经元替它们出力或者纠正它们的错误。可以理解为通过人为的对隐藏层添加噪声，加强模型的泛化能力。

## dropout实现

$$
\begin{align*}
z^{(l+1)}_i&=w_i(l+1)+b^{(l+1)}_i
\newline y^{(l+1)}_i&=f(z^{(l+1)}_i)
\end{align*}
$$

加上dropout后，将会变成

$$
\begin{align*}
r^{(l)}_j&\sim Bernoulli(p)
\newline\widetilde{y}^{(l)}&=r^{(l)}*y^{(l)}
\newline z^{(l+1)}_i&=w_i^{(l+1)}\widetilde{y}^l+b_i^{(l+1)}
\newline y_i{(l+1)} &=f(z_i^{(l+1)})
\end{align*}
$$

在训练的前向传播阶段，一些结点的输入会以p的概率设为1，1-p的概率为0，这样每一个batch都是一个dropout后的网络。在反向传播阶段，同样是对dropout后的网络进行梯度下降，被删掉的结点不更新参数。但在预测时，使用的是完整的网络，但权重都要乘上p。

## implementing dropout
传统dropout实现带来的问题是模型定义复杂，因为在测试阶段要定义不同的模型来完成权重的缩放，这样才能获得期望的结果。因此现在已经几乎不适用dropout，一般都使用implementing dropout（反向随机失活）

$$
\begin{align*}
r^{(l)}_j&\sim Bernoulli(p)
\newline\widetilde{y}^{(l)}&=r^{(l)}*y^{(l)}
\newline z^{(l+1)}_i&=(w_i^{(l+1)}/p)\widetilde{y}^l+b_i^{(l+1)}
\newline y_i{(l+1)} &=f(z_i^{(l+1)})
\end{align*}
$$

implementing dropout在训练阶段就对出现删除结点的层进行权重扩大，这样测试阶段就不需要纠结权重缩放问题，只需要禁用dropout就可以。

## 论文链接
> [A simple way to prevent neural networks from overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

