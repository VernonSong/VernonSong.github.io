---
layout: post
title:  机器学习学习笔记【3】
subtitle:  逻辑回归 过拟合
date: 2017-08-16 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml3.jpg"
catalog: true
tags:
    - 机器学习
---

### 逻辑回归
逻辑回归(Logistic Regression)是用于求解分类问题的一个简单高效的模型，其公式为：

$$
\begin{align*}& h_\theta (x) = g ( \theta^T x ) \newline \newline& z = \theta^T x \newline& g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}
$$

其中预测结果
$$
h_\theta (x)
$$
的值在0-1之间，很容易猜测出来，当
$$
h_\theta (x)
$$
为0.7时，就表明模型认为它有70%的可能是1，相应的，为0的概率就是1-0.7=0.3。
![](https://github.com/VernonSong/Storage/blob/master/image/ML4.png?raw=true)

#### 决策边界
决策边界(Decision Boundary)是假设函数
$$
h_\theta (x)
$$
的一个属性，它将整个数据空间一分为二，在边界一侧的我们预测为1，另一侧的预测为0。
![](https://github.com/VernonSong/Storage/blob/master/image/ML5.png?raw=true)

#### 代价函数
在逻辑回归中，同样需要代价函数来评判拟合的好坏，理论上来说，我们依然可以用所有模型误差的平方和这个定义，但问题在于，当我们将
$$
h(x) = \dfrac{1}{1 + e^{-_\theta^Tx}}
$$
带入到这个代价函数中时，得到的代价函数将是一个非凸函数(non-convex function)，这意味着我们有许多局部最小值，而梯度下降法找的是指定起点附近的局部最小值，因此就很难用梯度下降法找到其全局最小值。因此，在逻辑回归中，重新定义代价函数为：

$$
\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}
$$

这样构建的 Cost(hθ(x),y)函数的特点是：当实际的 y=1 且 hθ也为 1 时误差为 0，当 y=1，但 hθ不为 1 时误差随着 hθ 的变小而变大；当实际的 y=0 且 hθ 也为 0 时代价为 0，当 y=0，但 hθ不为 0 时误差随着 hθ 的变大而变大。

将其简化带入代价函数中得到：

$$
\begin{align*}& J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]\end{align*}
$$

使用这样的代价函数就可以用梯度下降法求能使代价函数最小的参数了，除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，比如：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)。这些算法虽然很复杂，但是通常不需要人工选择学习速率，效率也更高。

### 多类别分类
上面写的分类都有一个特点，就是结果不是1就是0，因此它们属于二元分类问题。但实际中有很多分类问题的类别不止一个，这类问题也可以用逻辑回归的想法去解决。

![](https://github.com/VernonSong/Storage/blob/master/image/ML6.png?raw=true)

在二元分类问题中，我们把某一类作为正向类，另一类作为负向类。那么，同样也可以想，把多个类别分为两类，一类是目前要预测的类，一类是其它类，我们用逻辑回归去预测是否是我们要预测的类。这样的分类器要覆盖到每一个类，类似上图，我们从三个分类器中选择概率最大的一个，就得到了结果。

### 过拟合
过拟合（Overfitting）现象是指在拟合一个统计模型时，使用过多参数，以至于太适应训练数据而非一般情况。这导致虽然模型能很好的匹配训练数据集里的数据，但是对于数据集之外的数据预测的结果就会出现很大的偏差。于此相对，欠拟合(Underfitting)就是使用过少的参数以至于不适应数据。
![](https://github.com/VernonSong/Storage/blob/master/image/ML6.png?raw=true)
#### 正则化
正则化(Regularization)是一种解决过拟合问题的手段，其原理是通过对参数θ添加惩罚措施，让代价函数尽可能小时，所需考虑的不仅仅是能不能尽可能匹配训练集的所有数据，同时还有各个θ值的大小。加入正则项后的代价函数公式如下：

$$
\begin{align*}& min_\theta\ \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2\end{align*}
$$

需要注意的是，我们惩罚的对象并不包括常数项。同时也要根据实际情况选择合理的正则化参数 λ。

正则化还有一个好处是，以前正规方程需要考虑
$$
X^TX
$$
在m<n时不可逆的问题，但将正则项带入正规方程后

$$
\begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \newline& \text{where}\ \ L = \begin{bmatrix} 0 & & & & \newline & 1 & & & \newline & & 1 & & \newline & & & \ddots & \newline & & & & 1 \newline\end{bmatrix}\end{align*}
$$

我们可以证明
$$
X^TX+\lambda \cdot L
$$
是可逆的。

