---
layout: post
title:  机器学习学习笔记【2】
subtitle:  特征缩放
date: 2017-08-6 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml2.jpg"
catalog: true
tags:
    - 机器学习
---

### 特征缩放
特征缩放(Feature scaling)可以理解为平常说的归一化。常用类似梯度下降这样算法前的特征处理中，因为当多个特征的范围差距过大时，梯度下降函数收敛的会非常慢，因此需要通过特征缩放，将特征的范围缩放到接近的范围，减少代价函数等高线图的倾斜，加快收敛。
![](https://github.com/VernonSong/Storage/blob/master/image/ml3.png?raw=true)
左边为没有进行特征缩放的代价函数等高线图，右边是进行了特征缩放的，可以看出右边能比左边更快的收敛。
在课程中讲的特征缩放方法是均值归一化。公式为：

$$
\begin{align*}& x' \leftarrow \frac{x-\mu}{max(x)-min(x)} \end{align*}
$$


其中μ为而均值。通过这种特征缩放方法将特征值都缩放至[-1,1]，使梯度下降法更好的工作，而其实归一化主要有两种方法：
#### min-max标准化(Min-Max Normalization)
它把原始数据映射到[0-1]之间，公式为：

$$
\begin{align*}& x' \leftarrow \frac{x-min(x)}{max(x)-min(x)} \end{align*}
$$


在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用这种归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。
#### 0均值标准化(z-score Normalization)
公式为：

$$
\begin{align*}& x' \leftarrow \frac{x-\mu}{ \sigma} \end{align*}
$$

其中，μ为均值，σ标准差。标准差是方差的开方。在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，此种方法表现更好。

### 正规方程
梯度下降法是通过迭代，不断逼近最小值的方法来获取其数值解。但其实，还有能直接求解析解的方法，此方法称为正规方程(Normal Equation)，不过搜索正规方程感觉内容较少，wiki百科和百度百科均找不到词条。

$$
\begin{align*}& \theta = (X^T X)^{-1}X^T y \end{align*}
$$

原理一知半解，就不细写。与梯度下降法相比，正规方程可以直接得到全局最优解，不需要选择α，迭代次数等，也不依赖特征缩放，但缺点是当特征维度n较大时，计算耗时长，因为其时间复杂度为O(n³)。并且当
$$
 X^T X
$$
不可逆时，我们通常需要通过删除不重要的，多余的特征或使用其他技术来解决训练样本小于参数。但总的来说，在特征数不多时，正规方程将会是更好的选择。

