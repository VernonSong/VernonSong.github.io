---
layout: post
title: 线性代数知识
subtitle:  深度学习数学基础
date: 2018-03-04 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-linear.jpg"
catalog: true
tags:
    - 数学基础
---
### 满轶矩阵
要理解满轶矩阵首先要知道矩阵的轶，一个矩阵A的列秩是A的线性独立的纵列的极大数。通常表示为r(A)，rk(A)或rank A。矩阵秩等于行数，称为行满秩，等于列数则为列满轶。而在方阵中，**列轶永远等于行轶**。满轶矩阵就是既是行满秩又是列满秩则为n阶矩阵。

### 逆矩阵
矩阵
$$
A
$$
的矩阵逆（matrix inversion）记作
$$
A^{-1}
$$
，其定义的矩阵满足如下条件

$$
\begin{align*}&
A^{-1}A=I_n
\end{align*}
$$
其中
$$
I_n
$$
为单位矩阵，利用任意向量与单位矩阵相乘，结果均不会改变的性质，可以做如下变换

$$
\begin{align*}
Ax=&b
\newline A^{-1}Ax=&A^{-1}b
\newline I_nx=&A^{-1}b
\newline x=&A^{-1}b
\end{align*}
$$

如果一个矩阵可逆，当且仅当它为满轶矩阵。

### 特征分解
特征分解是使用最广泛的矩阵分解之一，即将矩阵分解成一组特征向量和特征值。方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量v：

$$
Av=\lambda v
$$

标量
$$
\lambda
$$
被称为这个特征向量对应的**特征值**。

不是每个矩阵都可以分解成特征值和特征向量。每个实对称矩阵都可以分解成实特征向量和实特征值。

一个列向量线性相关的方针被称为**奇异**的，矩阵是奇异的当且仅当含有0特征值。

### 奇异值分解
奇异值分解（Singular Value Decomposition，SVD）是另一种矩阵分解方法，它将矩阵分解为**奇异向量**和**奇异值**：

$$
A=UDV^{\top}
$$

奇异值分解是一个很厉害的算法，可应用于图像压缩，噪声分离，推荐系统中。

### 范数
范数（norm），是具有“长度”概念的函数。是将向量映射到非负值的函数。形式上，
$$
L^p
$$
范数的定义如下

$$
\begin{align*}
||x||_p=(\sum_i|x_i|^p)^\frac{1}{p}  \quad p\in\mathbb{R},p\ge1
\end{align*}
$$

严格的来说，范数是满足如下条件的函数

$$
\begin{align*}
&f(x)=0\Rightarrow x=0
\newline &f(x+y)\leq f(x)+f(y)
\newline & \forall \alpha \in \mathbb{R},f(\alpha x)=|\alpha|f(x)
\end{align*}
$$

#### L²范数

$$
L^2
$$
范数又被称为**欧几里得范数**，从几何的角度上这个范数最好理解，表示从原点出发到向量x确定的点的欧几里得距离。
$$
L^2
$$
范数在机器学习中出现频繁，经常简化表示为||x||。此范数的矩阵形式称为弗罗贝尼乌斯（Frobenius）范数。

#### L¹范数
$$
L^1
$$
范数计算的是为向量中各元素绝对值之和，当机器学习问题零和非零元素之间的差异非常重要时，通常会使用
$$
L^1
$$
范数。此范数表达的距离又称为**曼哈顿距离**。

#### Lº范数
在一些地方也能看到
$$
L^0
$$
范数，将其带入公式，算出的是向量中非零元素的个数，但从最上面的定义中可以看出此术语在数学意义上是不对的，它不满足向量缩放α倍后范数也缩放α倍。

#### L∞范数
$$
L^\infty
$$
也被称为**最大范数**，这个范数表示向量中具有较大元素幅值的元素的绝对值：

$$
||x||_{\infty}=\max \limits_{i}|x_i|
$$

