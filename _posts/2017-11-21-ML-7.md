---
layout: post
title: 机器学习学习笔记【7】
subtitle: 支持向量机
date: 2017-11-21 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml7.jpg"
catalog: true
tags:
    - 机器学习
---

### 支持向量机思想简介

支持向量机（support vector machine）是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法，它在90年代中期发展起来，在当时风头甚至盖过了神经网络算法。

首先时经典的分类问腿：
![](https://github.com/VernonSong/Storage/blob/master/image/svm2-300x225.png?raw=true)
对于分类问题，如图中的两类，若采用黑线为分割，虽然能把两类正确的分离开来，但是其实分的不够稳妥，因为黑线离样本很近，当增大数据集时，如下图，可能就会出现错误分类的情况，而支持向量机，就是一个即使样本较少，也能有不错的分类效果的机器学习算法。

![](https://github.com/VernonSong/Storage/blob/master/image/svm3-300x225.png?raw=true)

还是上图的分类问题，如果想让这个分割线能把两类安全稳妥的分离开，直观的想法就是，希望分割线能距离样本远一点，如下图，这样，即使样本数目不算很多，但因为增大了分割线与离分割线最近的样本之间的距离，我们也能对其他数据正确分类。支持向量机正是运用这一思想，因此，它也被称为大距离分类器。
![](https://github.com/VernonSong/Storage/blob/master/image/svm4-300x225.png?raw=true)

我们把黄色区域的边叫做maximum margin，在maximum margin上的点叫做支持向量。可以看出因为与支持向量保持最大距离，在数据变多时我们的边界也能进行很好的分类。

![](https://github.com/VernonSong/Storage/blob/master/image/svm5-300x225.png?raw=true)
如果使用数学式子来表达，则我们的预测函数可以写为
$$
h_\theta(x)=g(\theta^Tx)
$$
其中
$$
\theta^Tx=\theta_0x_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n
$$
因为
$$
x_0=1
$$
，所以我们把式子写为
$$
h_\theta(x)=g(\theta^Tx)=g(w^Tx+b)
$$

这样
$$
w^Tx+b
$$
就很像直线的表达式，普通点到面的二维欧氏距离为
$$
\begin{align*}&
d=\dfrac{ax_1+bx_2+c}{\sqrt{a^2+b^2}}
\end{align*}
$$

其高维扩展就是点到超平面的距离，即几何间隔

$$
d=\dfrac{y(x)}{||w||}=\dfrac{x^Tx_0+b}{||w||}
$$

为了达到上面所说的，使离分割线最近的点与分割线距离尽可能大，我们要找出

$$
max\dfrac{min[y_i(x^Tx_i+b)]}{||w||}
$$

### 核函数

上面说到的都是线性可分的情况，对于线性不可分的情况，我们可以通过核函数将数据映射到高维空间

![](https://github.com/VernonSong/Storage/blob/master/image/svm7-300x167.png?raw=true)
![](https://github.com/VernonSong/Storage/blob/master/image/svm8-300x225.png?raw=true)

### 松弛变量

由于数据集本身的噪音，或者我们通过核函数映射过的数据依然有少量交叉，这都会导致我们无法通过一个超平面完全的分开两类，对此，支持向量机中引入了松弛变量（slack variable），其符号为
$$
\xi
$$
,我们通过对松弛变量进行惩罚来使超平面在允许误差的同时尽可能的保持最大间隔分类这一特性

新的目标函数为
$$
\begin{align*}&
min\dfrac{1}{2}||w||^2+\dfrac{\gamma}{2}\sum_{n=1}^N\xi_n^2
\end{align*}
$$

其中当分类错误时
$$
\xi>1
$$
,分类正确但在margin当中时，
$$
0<\xi<=1
$$
,分类正确且在margin之外，
$$
\xi=0
$$

### 多类别分类

因为一个SVM只能区分两类，所以对于多类别分类，我们需要进行多次分类，假设
可以对每个类做一次SVM，也可以两两做一次。

如果对每个类别进行分类，即对每个类用一个SVM判断数据是不是属于这个类，虽然对N个类别只需要做N次分类，但因为负样本数目远多于正样本，会造成类偏斜，导致分类效果变差。

如果每两个类别做一次分类，虽然解决了类偏斜问题，但N个类就需要有N(N-1)/2个分类器，也并非是一个完美的解决方案。