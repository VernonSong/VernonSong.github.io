---
layout: post
title: 机器学习学习笔记【9】
subtitle:   异常检测 推荐系统
date: 2017-12-25 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml9.jpg"
catalog: true
tags:
    - 机器学习
---

### 异常检测

#### 算法原理
谈到异常检测，通过之前的学习可能会先想到分类问题，既将无异常的作为正类，有异常的作为负类，使用二分类算法来解决。但实际我们会使用一个非监督学习来解决异常检测问题。

对于异常检测，绝大多数样本都集中在正常这一类中，所以其特征x的分布我们可以认为近似于高斯分布，样本是否正常的概率p表示如下

$$
\begin{align*}p(x,\mu,\sigma^2)=\dfrac{1}{\sqrt{(2\pi\mu)}}exp(-\dfrac{(x-\mu)^2}{2\sigma^2})\end{align*} 
$$

设定值ε为判定边界，当p高于ε时，我们认为样本正常，反之则认为是异常样本。

对给定数据集
$$
x^{(1)},x^{(2)},x^{(3)},...,x^{(m)}
$$
针对每一个特征，μ和σ²的值计算方式如下

$$
\begin{align*}
\mu=\dfrac{1}{m}\sum^m_{i=1}x^{(i)}\ \ \ \ \ \ \ \ \ \ \ \sigma^2=\dfrac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu)^2
\end{align*} 
$$

一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算

$$
\begin{align*}
p(x)=\prod^n_{j=1}p(x_j;\mu,\sigma^2)=\prod^n_{j=1}\dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j-\mu_j)^2}{2\sigma_j^2})
\end{align*} 
$$

因此，开发一个异常检测系统，我们所需要做的就是
1. 根据测试集数据，我们估计特征的平均值和方差并构建p(x)函数
2. 对交叉检验集，我们尝试使用不同的ε值作为阀值，并预测数据是否异常，根据 F1值或者查准率与查全率的比例来选择
3. 选出ε后，针对测试集进行预测，计算异常检验系统的 F1 值，或者查准率与查全率之比

#### 异常检测与监督学习对比


 异常检测       |     监督学习
:-------------------------:|:-------------------------:
非常少量的正向类（异常数据 y=1）,大量的负向类（y=0） |   同时有大量的正向类和负向类
许多不同种类的异常，非常难。根据非常少量的正向类数据来训练算法。  |有足够多的正向类实例，足够用于训练算法，
未来遇到的异常可能与已掌握的异常、非常的不同。|未来遇到的正向类实例可能与训练集中的非常近似。
用途：欺诈行为检测，生产（例如飞机引擎），检测数据中心的计算机运行状况  |  用途：邮件过滤器，天气预报，肿瘤分类


#### 多元高斯分布

假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。

下图中是两个相关特征，洋红色的线（根据 ε 的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的 x 所代表的数据点很可能是异常值，但是其p(x )值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。
![](https://github.com/VernonSong/Storage/blob/master/image/ml11.png?raw=true)
在一般的高斯分布模型中，我们计算p(x )的方法是： 通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算p(x )。

$$
\begin{align*}
&\Sigma=\dfrac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu)(x^{(i)}-\mu)^T=\dfrac{1}{m}(X-\mu)^T(X-\mu)
\newline&
p(x)=\dfrac{1}{(2\pi)^{\dfrac{n}{2}}|\Sigma|^{\dfrac{1}{2}}}exp\left(-\dfrac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
\end{align*} 
$$

协方差矩阵对模型的影响如下
![](https://github.com/VernonSong/Storage/blob/master/image/ml12.png?raw=true)
![](https://github.com/VernonSong/Storage/blob/master/image/ml13.png?raw=true)
原高斯分布模型和多元高斯分布模型的比较：


 原高斯分布模型       |     多远高斯分布模型
 :-------------------------:|:-------------------------:
不能捕捉特征之间的相关性，但可以通过人工将特征进行组合的方法来解决 |   自动捕捉特征之间的相关性
 计算代价低，能适应大规模的特征|计算代价较高 训练集较小时也同样适用
对数据无特殊要求 |  必须要有 m>n，不然的话协方差矩阵不可逆的，通常需要 m>10n 另外特征冗余，也会导致协方差矩阵不可逆


### 推荐系统
推荐系统是机器学习在商业中最显著的运用，它的目标尽可能正确的推荐给用户他们想看到的东西，例如购物网站将最新的键盘推荐给喜欢外设的用户。我们希望构建一个算法来预测用户会喜欢哪些东西。

#### 基于内容的推荐系统
在基于内容的推荐系统中，我们需要知道希望推荐的东西的一些特征，比如电影，我们希望知道它属于爱情片还是动作片。基于这些特征，我们采用线性回归模型

$$
\theta^{(j)}
$$
为用户j的参数向量

$$
x^{(j)}
$$
为电影i的特征向量

则对所有用户预测的代价函数为：

$$
\begin{align*}
min\dfrac{1}{2}\sum^{n_u}_{j=1}\sum_{i:r(i,j)=1}\left((\theta^{(j)})^Tx{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\sum^{n_u}_{j=1}\sum^n_{k=1}\left(\theta^{(j)}_k\right)^2
\end{align*} 
$$

如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：

$$
\begin{align*}
& \theta^{(j)}-\alpha\sum_{i:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})x^{(i)}_k \ \ \ \ (for \ \  k=0)
\newline
& \theta^{(j)}-\alpha\left(\sum_{i:r(i,j)=1}((\theta^{(j)})^T-y^{(i,j)})x^{(i)}_k+\lambda\theta^{(j)}_k\right) \ \ \ \ (for \ \  k\neq0)
\end{align*} 
$$

#### 协同过滤
在基于内容的推荐系统中，我们是根据已有的事物特征来训练模型。但如果我们没有这些特征，就需要使用协同过滤算法（Collaborative Filtering）。

还是继续线性回归模型，因为我们不知道
$$
x^{(j)}
$$
，因此优化目标便改为同时针对x和θ进行。

$$
\begin{align*}
J(x^{(1)},...x^{(n)},\theta^{(1)},...\theta^{(n)})=\dfrac{1}{2}\sum_{(i:j):r(i,j)=1}\left((\theta^{(j)})^Tx{(i)}-y^{(i,j)}\right)^2+\dfrac{\lambda}{2}\sum^{n_m}_{i=1}\sum^n_{k=1}\left(x_k^{(j)}\right)^2+\dfrac{\lambda}{2}\sum^{n_m}_{i=1}\sum^n_{k=1}\left(\theta^{(j)}_k\right)^2
\end{align*} 
$$

对代价函数求偏导

$$
\begin{align*}
& x^{(i)}_k:= x^{(i)}_k-\alpha\left(\sum_{j:r(i,j)=1}(\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta^{(j)}_k+\lambda x^{(j)}_k\right)
\newline
& \theta^{(i)}_k:= \theta^{(i)}_k-\alpha\left(\sum_{j:r(i,j)=1}(\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x^{(j)}_k+\lambda \theta^{(j)}_k\right)
\end{align*} 
$$

这样便能得出基于协同过滤的推荐系统模型，如果一位用户正在观看电影
$$
x^{(i)}
$$
，我们可以寻找另一部电影
$$
x^{(j)}
$$
，依据两部电影
的特征向量之间的距离
$$
||x^{(j)}-x^{(i)}||
$$
判断是否要给用户推荐电影
$$
x^{(j)}
$$
。
