---
layout: post
title:  机器学习学习笔记【4】
subtitle:  神经网络
date: 2017-08-16 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml4.jpg"
catalog: true
tags:
    - 机器学习
---

### 神经网络
对于线性回归或者逻辑回归，都存在一个缺点，就是在拟合时，随着特征数增加，参数项数目会呈指数级增长，即使我们只采用两两特征组合，参数项数目依然太多。这时我们就需要神经网络来解决。

神经网络虽然时现在热门的研究方向，但它其实是机器学习算法中的前辈了。神经网络兴起于二十世纪八九十年代，但由于各种原因，在90年代后期应用减少了，人们的研究方向转到了SVM等算法上。而如今，无论是对神经网络的理论研究还是计算机的计算能力都较之前有了很大的进步，这才导致了现在神经网络迅速的发展和广泛的应用。

#### 神经元和大脑
在对大脑的研究中，人们发现了一个有趣的现象，就是如果把耳朵到听觉皮层的神经切断，并重新接到一个动物的大脑上，这样从眼睛到视神经的信号最终将传到听觉皮层，听觉皮层将会学会"看“。由此推测，如果人体有同一块脑组织可以处理光，声或触觉信号，那么也许存在一种学习算法，可以同时处理视觉，听觉和触觉，而不是需要各种不同的程序。人们的目标就是找出这样一个类似大脑的学习方法。神经网络算法，也是模仿大脑中的神经网络的原理去构建的。

大脑中的神经网络是由无数神经元构成，神经元接收到信号后，它就会进行计算并把处理过的信号传递给其它神经元。神经网络模型同样也是建立在很多”神经元“上，这里的神经元也叫激活单元(Activation Unit)，每个神经元是一个学习模型。这些神经元采纳一些特征作为输入，并根据自身模型提供一个输出。

### 神经网络模型
![](https://github.com/VernonSong/Storage/blob/master/image/ML7.png?raw=true)
上图是一个3层的简单的神经网络模型，第一层为输入层（Input Layer），最后一层是输出层（Output Layer），中间的全部统称为隐藏层（Hidden Layers），隐藏层的层数越多，训练的计算就越复杂。在输入层和隐藏层中，需要增加偏差单位（Bias uint）。可以发现每一个a都是由上一层所有的x(和Θ所决定的。我们把这样从左到右的过程称为前向传播（Forward Propagation）。

从本质上讲， 神经网络能够通过学习得出其自身的一系列特征。在普通逻辑回归中我们被限制为使用数据中的原始特征，因此我们虽然可以用二项式项来组合这些特征，但仍受到这些原始特征的限制。而神经网络的原始特征只是输入层，我们可以在隐藏层中做一系列计算使其变为新的特征来帮助我们得到正确的结果。

![](https://github.com/VernonSong/Storage/blob/master/image/ML8.png?raw=true)

在神经网络中，当我们有不只两种分类时，比如上图分类路人，汽车，摩托车和卡车，在输出层我们就应该有4个值，每个值是神经网络对输入是否是这一类所做出的预测。
