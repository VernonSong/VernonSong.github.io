---
layout: post
title:  R-CNN系列网络小结
subtitle:  
date: 2018-08-31 20:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-rcnn.jpg"
catalog: true
tags:
    - 深度学习
    - 计算机视觉
---

## R-CNN
在R-CNN网络出现前，使用CNN进行目标检测主要有两种方法，一种是将边框的定位看做是一个回归问题，一种是使用滑动窗口检测，但两种方法效果都不好。因此，R-CNN采用了新的方法，先通过一个传统的选择性搜索（selective search）算法来获取建议区域（region proposal），然后将每个建议区域缩放至固定大小（227*227），并使用预训练的AlexNet将区域转化为4096维的特征向量。最后将特征向量送入SVM中，对其进行分类。由于这样得到的边框并不够精准，因此还需通过边框回归（Bounding Box Regression）来校准边框，提升准确度。

在训练时，R-CNN使用IoU重叠阈值来作为判断正反例的依据，由于CNN网络对小样本容易过拟合，需要在训练CNN网络时降低IoU的阈值，将那些并不精确的区域也做为正样本。而由于SVM网络对样本数不敏感，因此可以提高IoU阈值以提高准确率。在训练边框回归时同样要基于IoU，只训练大于某一IoU阈值的，因为只有这样才符合矫正边框这一目的，同时使模型接近线性，而与真实边界偏离较大的则没有矫正的必要。

### 边框回归
R-CNN进行边框调整的思路非常简单，就是平移加缩放，

$$
\begin{align*}
&\hat{G_x}=P_wd_x(P)+P_x
\newline &\hat{G_y}=P_hd_y(P)+P_y
\newline &\hat{G_w}=P_w \mathrm{exp}(d_w(P))
\newline &\hat{G_h}=P_h \mathrm{exp}(d_h(P))
\end{align*}
$$

其中$d_x(P),d_y(P),d_w(P),d_h(P)$为边框回归需要去学习的目标，之所以要在平移的时候需要乘上宽高，是为了消除尺度的影响，这样不管目标是大是小，回归函数都能学习到平移的参数。而由于宽高需要大于0，因此使用$\mathrm{exp}$来讲缩放因子的数值空间转换到非负数。

```python
  # 预测的中心坐标和宽高
  pred_ctr_x = tf.add(tf.multiply(dx, widths), ctr_x)
  pred_ctr_y = tf.add(tf.multiply(dy, heights), ctr_y)
  pred_w = tf.multiply(tf.exp(dw), widths)
  pred_h = tf.multiply(tf.exp(dh), heights)
```

而函数的目标值为

$$
\begin{align*}
&t_x=(G_x-P_x)/P_w
\newline &t_x=(G_y-P_y)/P_h
\newline &t_w=\mathrm{log}(G_w/P_w)
\newline &t_h=\mathrm{log}(G_h/P_h) 
\end{align*}
$$

```python
  targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths
  targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights
  targets_dw = np.log(gt_widths / ex_widths)
  targets_dh = np.log(gt_heights / ex_heights)
```

在R-CNN中，使用L2loss来进行优化，完成边框回归操作。
 



## SPPNets
由于R-CNN网络需要对选择性搜索所选出的2000个候选窗口分别使用CNN来获取特征，这些计算大部分都是重复计算，使网络速度极慢。因此SPPNet提出只在整张图片上提取一次特征，然后通过空间金字塔池化（Spatial Pyramid Pooling，SPP）层，将每个建议区域的特征图转化为固定大小的特征向量。经对比，SPPnet的速度比R-CNN提升了数十倍，并且准确率与之接近。

### 空间金字塔池化层
空间金字塔池化是一种词袋模型（Bag-of-Words, BoW）的扩展，它将图像切分成粗糙到精细各种级别，然后整合特征。在CNN出现前，SPP一直是许多计算机视觉系统中的核心组件。SPPnet作者将SPP应用于深度CNN当中，进行多级池化，提升网络对物体变形的鲁棒性。并根据输入大小不同，动态变化池化的窗口大小和步长，使SPP的输出尺寸始终是固定的，避免了因全连接层需输入固定大小的特征而导致的图像缩放，保留了最原始的信息。

![img](/img/in-post/post-object-detection/post-object-detection_1.png)

```python
def spatial_pyramid_pool(net,out_pooling_size):
    batch_size, h_in, w_in, feature_size = net.shape
    for i in range(len(out_pooling_size)):
        k_h = s_h = math.ceil(h_in/out_pooling_size[i][0])
        k_w = s_w = math.ceil(w_in/out_pooling_size[i][1])
        p_h = math.floor((k_h*out_pooling_size[i][0]-h_in+1)/2)
        p_w = math.floor((k_w * out_pooling_size[i][1] - w_in + 1) / 2)
        net_tmp = tf.pad(net, tf.constant([[0, 0], [0, p_h], [0, p_w], [0, 0]]))
        pooling = slim.max_pool2d(net_tmp, (k_h, k_w), stride=(s_h, s_w), padding='SAME', scope='SPP_'+str(i))
        pooling = tf.reshape(pooling, [batch_size, -1, feature_size])
        if i == 0:
            spp = pooling
        else:
            spp = tf.concat([spp, pooling], 1)
    return spp


net = np.zeros((128, 9, 7, 3), dtype=np.float32)
spatial_pyramid_pool(net, [[4, 4], [2, 2]])
```


## Fast R-CNN
虽然SPPnets极大的提升了训练和识别的速度，但它依然没有脱离R-CNN多阶段流水线式的结构，训练复杂，并且特征还需写入硬盘。除此之外，由于SPPnets反向传播难以更新SPP之前的卷积层，导致网络准确率受限。

Fast R-CNN在R-CNN的基础之上，借鉴了SPPnets的思路，对整张图片进行特征提取，并用一个单级的SPP层来将建议区域的特征图转化为固定大小。作者将这个特殊SPP层称为RoI（Region of Interest, RoI）池化层。Fast R-CNN还使用了多任务损失，将分类与边框回归合并在一起，精简了模型。

由于SPPnet是先把所有图像用选择性搜索所的到的的RoIs存起来，再从中每次一个batch的RoIs进行训练，但这一个batch的RoIs可能来自许多张图像，为了能够反向传播，需要将CNN对这些图像计算的所有数据存储起来，时间和空间上开销都比较大，网络较深时难以进行反向传播；而Fast R-CNN每次只选N张图像的RoIs生成batch，因此训练时每次只要计算和存储N张图像的在前向传播中所计算的数据，所以时间和内存开销更小。在实验中，N=2和R（batch size）=128时，达到了很好的成绩。


### Fast RCNN loss
Fast R-CNN有两个输出层，一个用来进行类别预测，输出概率$p$和类别$u$；一个进行边框回归，对这两个任务进行联合训练：

$$
L(p, u, t^u, v) = L_{cls}(p, u) + \lambda[u ≥ 1]L_{loc}(t^u,v)
$$

其中分类损失函数为：

$$
 L_{cls}(p, u)=- \mathrm{log}p_u
$$



回归损失为：

$$
L_{loc}(t^u,v)=\sum_{i \in \lbrace x,y,w,h \rbrace  } \mathrm {smooth}_{L_1}(t_i^u-v_i)
$$

与R-CNN不同的是，Fast R-CNN在预测值与目标值相差很大时使用$L_1$loss，比较鲁棒，有效避免了梯度爆炸。

$$
\mathrm{smooth}_{L_1}(x)=\begin{cases}
\begin{aligned}
0.5x^2  \quad & \mathrm{if}|x|<1
\newline   |x|-0.5  \quad & \mathrm{otherwise}
\end{aligned}
\end{cases}
$$

```python
def smooth_l1_loss(bbox_pred, bbox_targets, dim=[1]):
    box_diff = bbox_pred - bbox_targets
    abs_in_box_diff = tf.abs(box_diff)
    # 判断是否大于1
    smoothL1_sign = tf.stop_gradient(tf.to_float(tf.less(abs_in_box_diff, 1)))
    loss_box = tf.pow(box_diff, 2) * 0.5 * smoothL1_sign \
                  + (abs_in_box_diff - 0.5) * (1. - smoothL1_sign)
    loss_box = tf.reduce_mean(tf.reduce_sum(loss_box,axis=dim ))
    return loss_box
```

## Faster R-CNN
Fast R-CNN还有个计算瓶颈在于选择性搜索较为耗时，未能利用GPU进行高效的计算。因此作者设计了一个区域提议网络（RegionProposal Network，RPN)来计算推荐区域，使Fast R-CNN的速度和准确率都得到了提升。

![img](/img/in-post/post-object-detection/post-object-detection_2.png)

### RPN
RPN网络引入了锚点（Anchor）来提供建议区域。假设特征图上每个位置可能提议的最大数目为$k$，则总共会产生$H \times W \times k$个锚点，通过对这些Anchor进行边框回归，得到最终的建议区域。为了让网络更好的处理多尺度问题，在Faster R-CNN中锚点有3种面积（128\*128,256\*256，512\*512），三种长宽比（1:2,1:1,2:1），这样每个点就有9种尺度锚点，即$k=9$。

```python
def _whctrs(anchor):
  """
  Return width, height, x center, and y center for an anchor (window).
  """
  w = anchor[2] - anchor[0] + 1
  h = anchor[3] - anchor[1] + 1
  x_ctr = anchor[0] + 0.5 * (w - 1)
  y_ctr = anchor[1] + 0.5 * (h - 1)
  return w, h, x_ctr, y_ctr


def _mkanchors(ws, hs, x_ctr, y_ctr):
  """
  Given a vector of widths (ws) and heights (hs) around a center
  (x_ctr, y_ctr), output a set of anchors (windows).
  """

  ws = ws[:, np.newaxis]
  hs = hs[:, np.newaxis]
  anchors = np.hstack((x_ctr - 0.5 * (ws - 1),
                       y_ctr - 0.5 * (hs - 1),
                       x_ctr + 0.5 * (ws - 1),
                       y_ctr + 0.5 * (hs - 1)))
  return anchors

def _ratio_enum(anchor, ratios):
  """
  Enumerate a set of anchors for each aspect ratio wrt an anchor.
  """
  w, h, x_ctr, y_ctr = _whctrs(anchor)
  size = w * h
  size_ratios = size / ratios
  ws = np.round(np.sqrt(size_ratios))
  hs = np.round(ws * ratios)
  # 得到面积一样，长宽比不同的anchors
  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
  return anchors

def _scale_enum(anchor, scales):
  """
  Enumerate a set of anchors for each scale wrt an anchor.
  """
  w, h, x_ctr, y_ctr = _whctrs(anchor)
  ws = w * scales
  hs = h * scales
  # 得到面积不同的anchors
  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)
  return anchors

def generate_anchors(base_size=16, ratios=[0.5, 1, 2],
                     scales=2 ** np.arange(3, 6)):
  """
  Generate anchor (reference) windows by enumerating aspect ratios X
  scales wrt a reference (0, 0, 15, 15) window.
  """
  base_anchor = np.array([1, 1, base_size, base_size]) - 1
  ratio_anchors = _ratio_enum(base_anchor, ratios)
  # 对每个archor再扩展出3种面积（128^2,256^2,512^2)
  print(ratio_anchors)
  anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)
                       for i in range(ratio_anchors.shape[0])])
  return anchors
```

因此RPN有两个任务，一个判断该Anchor是否是目标，一个是Anchor的边框进行回归。RPN使用两个1*1的卷积层来完成这两个任务的预测，使建议区域继承了Anchor的平移不变性，并减少了参数数量。

![img](/img/in-post/post-object-detection/post-object-detection_3.png)

```python
def reshape_layer(bottom, num_dim, name):

    with tf.variable_scope(name) as scope:
        batch_size = bottom.get_shape()[0].value
        height = bottom.get_shape()[1].value
        width = bottom.get_shape()[2].value
        features = bottom.get_shape()[3].value
        # change the channel to the caffe format
        to_caffe = tf.transpose(bottom, [0, 3, 1, 2])
        reshaped = tf.reshape(to_caffe,[batch_size,
                                        num_dim,
                                        int(float(height * width) / float(num_dim)),
                                        features])
        # then swap the channel back
        to_tf = tf.transpose(reshaped, [0, 2, 3, 1])
        return to_tf
        
        
rpn = slim.conv2d(net, fliters, 3, stride=1, padding='SAME', activation_fn=tf.nn.relu, scope="rpn_conv/3x3")
# rpn 二分类
rpn_cls_score = slim.conv2d(rpn, num_anchors * 2, 1, padding='VALID', activation_fn=None, scope='rpn_cls_score')
# 转化为[1,h*9,w,2]
rpn_cls_score_reshape = reshape_layer(rpn_cls_score, 2,'rpn_cls_score_reshape')
# softmax激活
rpn_cls_prob_reshape = tf.nn.softmax(rpn_cls_score_reshape)
# 转化为[1,h,w,18]
rpn_cls_prob = reshape_layer(rpn_cls_prob_reshape, num_anchors * 2,'rpn_cls_prob_reshape')
# rpn 边框回归
rpn_bbox_pred = slim.conv2d(rpn, num_anchors * 4, 1, padding='VALID', activation_fn=None, scope='rpn_bbox_pred')
```

在得到$H \times W \times k$个Anchor之后，还需对这些Anchor进行处理和筛选，首先对出界的边框进行裁剪。

```python
def clip_boxes_tf(boxes, im_info):
  b0 = tf.maximum(tf.minimum(boxes[:, 0], im_info[1] - 1), 0)
  b1 = tf.maximum(tf.minimum(boxes[:, 1], im_info[0] - 1), 0)
  b2 = tf.maximum(tf.minimum(boxes[:, 2], im_info[1] - 1), 0)
  b3 = tf.maximum(tf.minimum(boxes[:, 3], im_info[0] - 1), 0)
  return tf.stack([b0, b1, b2, b3], axis=1)
```

之后使用非极大值抑制算法（Non-Maximum Suppression，NMS）去除重叠度高的建议区域，再将这些区域按照得分从高到底排序，留下得分最高的2000的建议区域，完成RPN最后的筛选工作。


## 参考
> [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)
> <br/>
> [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](https://arxiv.org/abs/1406.4729)
> <br/>
> [Fast R-CNN](https://arxiv.org/abs/1504.08083)
> <br/>
> [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)
> <br/>
> [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn)



