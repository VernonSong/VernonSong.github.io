---
layout: post
title:  机器学习学习笔记【5】
subtitle:  反向传播
date: 2017-08-16 09:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-ml5.jpg"
catalog: true
tags:
    - 机器学习
---

### 反向传播算法

对于一个神经网络结构，从输入层开始到隐藏层最后到输出层的计算过程构成了前向传播，在前向传播完成后，我们还需要根据损失函数来调整参数，在神经网络中通常使用反向传播算法（BackPropagation），因为此算法在神经网络中非常重要，因此认真推导一遍，也借着对反向传播算法推导理解的过程来复习神经网络结构。

#### 前向传播过程

![](https://github.com/VernonSong/Storage/blob/master/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%80%E5%8D%95.png?raw=true)
我们把上图的结构看成一个超小型神经网络，运算得到结果的过程是一个前向传播

$$
\begin{align*}& a_1^{(2)} = g(z_1^{(2)})=g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2)  \newline& a_2^{(2)} = g(z_2^{(2)})=g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2) \newline& h_\Theta(x)  =g(z_1^{(3)})= g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)})  \end{align*}
$$

<br>
激活函数均为sigmoid函数
$$
g(z)=\dfrac{1}{1+e^{-z}}
$$

#### 反向传播+梯度下降

得到结果后我们开始计算误差

$$
E_{total}= \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$

我们发现网络还需继续调整，因此想研究
$$
\Theta_{11}^{(2)}
$$
对整体的误差有多大的影响，于是用整体误差对
$$
\Theta_{11}^{(2)}
$$
求偏导数，并根据链式法则把式子写成

$$
\begin{align*}
 \dfrac{\partial E_{total}}{\partial  \Theta_{11}^{(2)}}= \dfrac{\partial E_{total}}{\partial h_\Theta(x)}*\dfrac{\partial  h_\Theta(x)}{\partial z_1^{(3)}} * \dfrac{\partial z_1^{(3)}}{\partial \Theta_{11}^{(2)}}
\end{align*}
$$

我们可以理解为，因为
$$
\Theta_{11}^{(2)}
$$
这个参数不够好导致算出来的
$$
z_1^{(3)}
$$
产生了偏差，而
$$
z_1^{(3)}
$$
会通过激活函数把偏差带给
$$
h_\Theta(x)
$$
，也就是最终结果，从而改变了
$$
E_{total}
$$
，这个过程像是误差通过一个锁链传递过去。环环相扣。

现在我们计算每个式子的值

$$
\begin{align*}&
 \dfrac{\partial E_{total}}{\partial h_\Theta(x)}= y-h_\Theta(x)
\newline&
\dfrac{\partial  h_\Theta(x)}{\partial z_1^{(3)}}=h_\Theta(x)*(1-h_\Theta(x))
\newline&
\dfrac{\partial z_1^{(3)}}{\partial \Theta_{11}^{(2)}}=a_1^{(2)}
\end{align*}
$$

三个结果相乘即为
$$
 \dfrac{\partial E_{total}}{\partial  \Theta_{11}^{(2)}}
$$

这样我们就可以用梯度下降来调整该参数
$$
\begin{align*}&
\Theta_{11}^{(2)}:=\Theta_{11}^{(2)}- \alpha\dfrac{\partial E_{total}}{\partial  \Theta_{11}^{(2)}}
\end{align*}
$$

这样就通过反向传播+梯度下降完成了对
$$
\Theta_{11}^{(2)}
$$
的一次调整

![](https://github.com/VernonSong/Storage/blob/master/image/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A8.png?raw=true)
同理，在一个神经元多一点的网络中，对
$$
\Theta_{11}^{(2)}
$$
的调整过程路径如下，可以看出反向传播要经过所有被
$$
\Theta_{11}^{(2)}
$$

$$
\begin{align*}&
 \dfrac{\partial E_{total}}{\partial  \Theta_{11}^{(3)}}= (\dfrac{\partial E_{total}}{\partial h_\Theta(x)_1}*\dfrac{\partial  h_\Theta(x)_1}{\partial z_1^{(4)}}) * \dfrac{\partial z_1^{(4)}}{\partial \Theta_{11}^{(3)}}
\newline&
 \dfrac{\partial E_{total}}{\partial  \Theta_{21}^{(3)}}= (\dfrac{\partial E_{total}}{\partial h_\Theta(x)_2}*\dfrac{\partial  h_\Theta(x)_2}{\partial z_2^{(4)}} )* \dfrac{\partial z_2^{(4)}}{\partial \Theta_{21}^{(3)}}
\newline&
  \dfrac{\partial E_{total}}{\partial  \Theta_{11}^{(2)}}= (\dfrac{\partial E_{total}}{\partial h_\Theta(x)_1}*\dfrac{\partial  h_\Theta(x)_1}{\partial z_1^{(4)}} )* \dfrac{\partial z_1^{(4)}}{\partial a_1^{(3)}} * \dfrac{\partial  a_1^{(3)}}{\partial z_1^{(3)}}* \dfrac{\partial z_1^{(3)}}{\partial \Theta_{11}^{(2)}} +
 (\dfrac{\partial E_{total}}{\partial h_\Theta(x)_2}*\dfrac{\partial  h_\Theta(x)_2}{\partial z_2^{(4)}} )* \dfrac{\partial z_2^{(4)}}{\partial a_1^{(3)}} * \dfrac{\partial  a_1^{(3)}}{\partial z_1^{(3)}}* \dfrac{\partial z_1^{(3)}}{\partial \Theta_{11}^{(2)}}
\end{align*}
$$

可以看出，在反向传播时，下一层所做的计算不过是前一层计算过的项乘上新的偏导项并求和，可以看作一个动态规划的过程，我们可以使用前面计算的结果，保证每个路径只访问一次，避免了重复计算，使时间复杂度大大降低。

#### 反向传播特点总结

反向传播的优点有：
- 快速方便，容易理解和实现
- 反向传播学习不需要输入向量的标准化（normalization）；然而，标准化可提高性能

其缺点是
- 从反响传播学习获得的收敛很慢
- 容易陷入局部最小
