---
layout: post
title:  SE模块原理及实现
subtitle: 
date: 2018-04-27 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-senet.jpg"
catalog: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
SENet是在2017年ImageNet竞赛中斩获第一的网络模型，该网络通过引入SE模块（Squeeze-and-Excitation）来让网络学习每个特征通道的重要程度，以较小的计算开销换来巨大的效果提升，且该模块可应用于其他主流CNN网络中。


## 原理
CNN网络通常有一系列的卷积层，非线性层和下采样层构成，并由此获得全局感受野来分层捕获图像特征，最近的研究表明优化空间维度的信息能提升网络的性能。此论文从通道间关系的角度着手，引入SE模块，通过显式建模，让网络自适应的对通道重要程度进行校准。

SE模块的结构如下图所示，其中X→U的过程为一个或一组卷积操作，最关键的是后面的Squeeze（挤压）与Excitation（激励）两个部分

![](https://github.com/VernonSong/Storage/blob/master/image/SE1.png?raw=true)

### Squeeze：植入全局信息

Squeeze操作将全局空间信息压缩到一个通道描述符中，通过全局平均池化来统计通道信息，其公式为：

$$
\begin{align*}
z_c=F_{sq}(u_c)=\frac{1}{H\times W}\sum^H_{i=1}\sum^W_{j=1}u_c(i,j)
\end{align*}
$$

通过全局平均池化，将二维的特征信息变为实数，此实数在某种程度上具有全局感受野，当然，也可以使用更复杂的方法获得此实数。

### Excitation：自适应校准
Excitation操作将使用Squeeze得到信息来捕捉通道间联系，论文提出两点要求：

1. 它必须能够捕捉通道间的非线性关系
2. 它必须学习一个非互斥的关系来确保允许强调多个通道而并非独热激活

为满足上面的要求，作者使用了带有sigmoid激活的门机制

$$
\begin{align*}
s=F_{ex}(z, W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))
\end{align*}
$$

其中σ为ReLU函数，之后连接两个全连接层，一个降维层，一个升维层。
$$
W_1
$$
为降维层参数，
$$
r
$$
为降维比率，通过ReLU函数激活并用参数为
$$
W_2
$$
的全连接层升维。经过Sigmoid函数激活后，得到范围在0~1的
$$
s
$$
，可以将
$$
s
$$
理解为每个特征通道的权重，将此权重与对应的特征通道相乘便可得到校准后的特征。

$$
\begin{align*}
& \tilde{x}_c=F_{scale}(u_c,s_c)=s_c \cdot u_c
\newline &\tilde{X}=[\tilde{x}_1,\tilde{x}_2,...,\tilde{x}_c,]
\end{align*}
$$

**相比于连接一个全连接层，通过一个降维层和一个升维层，可以减少参数，同时使网络具有更多的非线性特征。**

## SE模块计算性能
理论上SE模块只需要很小的计算开销，就可以提升网络效果，以ResNet-50为例，当输入图片大小为224*224，降维比例为16时，理论上SE-ResNet-50仅需要0.26%的额外计算开销，便达到了接近ResNet-101的性能。但在实际运行时，由于GPU没有对全局池化和较小计算量的全连接层进行优化，SE-ResNet-50要比ResNet-50耗时增加10%左右。

SE模块所带来的额外参数数目可用下面的式子表示：

$$
\begin{align*}
\frac{2}{r}\sum^S_{s=1}N_s\cdot {C_s}^2
\end{align*}
$$

$$
r
$$
为降维比例，
$$
S
$$
为阶段，
$$
C_s
$$
为阶段为
$$
s
$$
时的特征通道数，
$$
N_s
$$
为该阶段重复次数。

以SE-ResNet-50为例，相比于ResNet-50，SE-ResNet-50的额外参数主要在最后一个阶段，因为最后阶段的特征通道数目最多。去掉此阶段的SE模块，可以在对网络性能影响很小的同时将额外参数减小到4%。

## 不同阶段SE模块影响
在CNN网络中，浅层部分主要提取一些简单的特征，深层部分学习更复杂的特征。而SE模块在网络的不同阶段有着不同的作用。
![](https://github.com/VernonSong/Storage/blob/master/image/SE2.png?raw=true)
在网络前期，不同类别的分布在较低层中基本相同，特征通道的重要性由所有类别共享，但随着网络加深，不同的通道变得更加专业，它们对不同的类别有着不用的分工。由此推测在网络前期，SE模块帮助强化更有用的信息，在网络后期，SE模块提升网络特征提取的特异性。

而SE_5_2和SE_5_3虽是网络后期，但未展现出不同通道对不同类别的差异性，推测SE模块对此部分重要性不如在以前的块重要。这一发现与前面去掉最后阶段的SE模块，网络性能变化不大的试验结果相符。


## SE模块实现

### Keras实现

```python
def squeeze_excitation_block(x, out_dim,ratio): 
    
    #Squeeze
    squeeze=GlobalAveragePooling2D()(x)
    
    #Excitation
    dim=int(out_dim/ratio) #降维比例
    excitation=Dense(dim,activation='relu')(squeeze) #降维 ReLU激活
    excitation=Dense(out_dim,activation='sigmoid')(excitation) #升维 Sigmoid激活
    excitation = Reshape( (1,1,out_dim))(excitation)
    scale=Multiply()([x,excitation]) #得到校准后的特征
    
    return scale
```

### Tensorflow实现

```python
def Squeeze_excitation_block(self, x, out_dim, ratio, layer_name):
    
    with tf.name_scope(layer_name) :  
        
        #Squeeze
        squeeze = global_avg_pool(x, name='Global_avg_pooling')
        
        #Excitation
        dim=int(out_dim/ratio) #降维比例
        excitation = tf.layers.dense(squeeze, use_bias=True,units=dim,layer_name=layer_name+'_fully_connected1') #降维
        excitation = tf.nn.relu(excitation) #ReLU激活
        excitation = tf.layers.dense(excitation, use_bias=True,units=out_dim,layer_name=layer_name+'_fully_connected2') #升维
        excitation = tf.nn.sigmoid(excitation) #Sigmoid激活
        excitation = tf.reshape(excitation, [-1,1,1,out_dim])
        scale = x * excitation #得到校准后的特征
    
        return scale
```

## 链接
### 论文
>[Squeeze-and-Excitation Networks](https://arxiv.org/pdf/1709.01507.pdf)
