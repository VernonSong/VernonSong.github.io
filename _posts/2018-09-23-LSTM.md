---
layout: post
title:  LSTM原理与实现
subtitle: 
date: 2018-07-13 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-lstm.jpg"
catalog: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
RNN的最重要的特点就在于它能结合之前时间步的信息到当前时间步，以此来更好的解决时序问题。但当时间间隔很远时，普通的RNN网络便无法连接这两个时间步的信息。为解决这样的**长期依赖**问题，人们研发出了长短时记忆网络。

## 原理

### 常数误差流
解决长期依赖的根本就是让误差能顺利传到多个时间步以前，由于RNN的BPTT是一个随时间累乘的过程，随着时间步变长容易出现梯度爆炸或者梯度消失现象。


因此我们希望梯度流恒等于1，为了获得这样的常数误差流（constant error carousel ,CEC），线性激活函数是一个非常好的选择，而tanh函数具有在输入的绝对值较小时输出近似线性的性质。最重要的是通过加性操作将信息存储，这样LSTM单元为：

$$
\begin{align*}
&s_t=W_\mathbf{s}x_t+U_\mathbf{s}h_{t-1}+b_\mathbf{s}
\newline &C_t=C_{t-1}+s_t
\newline & h_t=tanh(C_t)
\newline &\hat{y}=softmax(Vh_t+b_{\mathbf{y}})
\end{align*}
$$

此时$\frac{\partial C_{t}}{\partial C_{t-1}}=1$，误差能无损向前传播。

### 输入门
我们希望能通过$s$将信息更新进$C$中，并让$C$保持此状态，但由于所有时间步的共享权值的特性，对$s$来说更新$C$与保持$C$不变是矛盾的，因此需要通过门机制来控制每个时间步更新哪些信息到$C$中。

$$
\begin{align*}
& i_t=\sigma (W_\mathbf{i}h_{t-1}+U_\mathbf{i}x_t+b_\mathbf{i}) 
\end{align*}
$$

$i_t$为每个信息的权重，通过sigmoid函数将其归一化到0-1之间，以此来控制传入到$C$中的信息流。

### 输出门
由于$C$为长期记忆，很多信息是存储来以后使用，都作为当前时刻的输出显然不合理，所以我们同样需要一个门机制来控制输出的信息流。

$$
\begin{align*}
& o_t=\sigma (W_\mathbf{o}h_{t-1}+U_\mathbf{o}x_t+b_\mathbf{o})
\end{align*}
$$

### 遗忘门
在tanh函数激活的情况下，一直增加$C$很容易让$h$的值趋近饱和，这样继续增加$C$意义不大，因此我们还需要一个门来控制何时去”遗忘一些记忆”。

$$
\begin{align*}
f_t=\sigma (W_\mathbf{f}h_{t-1}+U_\mathbf{f}x_t+b_\mathbf{f})
\end{align*} 
$$

### 反向传播

$$
\begin{align*}
&s_t=W_\mathbf{s}x_t+U_\mathbf{s}h_{t-1}+b_\mathbf{s}
\newline &C_t=f_t*C_{t-1}+i_t*tanh(s_t)
\newline & h_t=o_ttanh(C_t)
\newline &\hat{y}=softmax(Vh_t+b_{\mathbf{y}})
\end{align*}
$$

由$C$所导致的误差由后一时间步传回来的的误差与当前时间步误差两部分组成

则可推导

$$
\begin{align*}
\delta _\mathbf{C}^{(t)} =&\frac{\partial E}{\partial C_{t}}=\frac{\partial E}{\partial C_{t+1} }\frac{\partial C_{t+1}}{\partial C_{t}}+\frac{\partial E}{\partial h_{t} }\frac{\partial h_{t}}{\partial C_{t}}
\newline =&\delta _\mathbf{C}^{(t+1)}f_t+\delta _\mathbf{h}^{(t)} \odot o_t \odot (1-tanh^2(c_t))
\newline \delta _\mathbf{i}^{(t)} = & \frac{\partial E}{\partial i_{t}}=\frac{\partial E}{\partial C_{t} }\frac{\partial C_{t}}{\partial i_{t}} =\delta _\mathbf{C}^{(t)} \odot s_t
\newline \delta _\mathbf{f}^{(t)} =& \frac{\partial E}{\partial f_{t}}=\frac{\partial E}{\partial C_{t} }\frac{\partial C_{t}}{\partial f_{t}} =\delta _\mathbf{C}^{(t)} \odot C_{t-1}
\newline \delta _\mathbf{o}^{(t)} =& \frac{\partial E}{\partial o_{t}}=\frac{\partial E}{\partial h_{t} }\frac{\partial h_{t}}{\partial o_{t}} =\delta _\mathbf{h}^{(t)} \odot tanh(C_t)
\newline \delta _\mathbf{h}^{(t)} =& \frac{\partial E}{\partial h_{t}}=V^T(\hat{y}_t-y_t)
\end{align*}
$$

由此可得到LSTM各个参数的梯度，LSTM的方向传播计算时间复杂度为$O(1)$

### Peephole
这样的LSTM结构还有一个问题，由于所有的门的输入都是上个时间步的输出和当前时间步的输入，当输出门关闭时，就会导致记忆信息$c$被截断，门变成了单纯由当前时间步的输入所控制，这与CEC的想法背道而驰。

为了能让每个门都能从$c$中获取信息，研究者提出了Peephole机制（猫眼），让每个门都能直接读取$c$中信息：

$$
\begin{align*}
& i_t=\sigma (W_\mathbf{i}h_{t-1}+U_\mathbf{i}x_t+V_\mathbf{i}c_{t-1}+b_\mathbf{i}) 
\newline &f_t=\sigma (W_\mathbf{f}h_{t-1}+U_\mathbf{f}x_t+V_\mathbf{f}c_{t-1}+b_\mathbf{f})
\newline & o_t=\sigma (W_\mathbf{o}h_{t-1}+U_\mathbf{o}x_t+V_\mathbf{o}c_{t}+b_\mathbf{o})
\end{align*}
$$

## 实现
### Tensorflow API

#### tf.nn.rnn_cell.LSTMCell
```python
@tf_export("nn.rnn_cell.LSTMCell")
class LSTMCell(LayerRNNCell):
  def __init__(self, num_units,
                use_peepholes=False, cell_clip=None,
                initializer=None, num_proj=None, proj_clip=None,
                num_unit_shards=None, num_proj_shards=None,
                forget_bias=1.0, state_is_tuple=True,
                activation=None, reuse=None, name=None, dtype=None, **kwargs):
```

**Args:**：
- **num_units**：LSTM细胞单元数
- **use_peepholes**：是否使用猫眼
- **cell_clip**：LSTM细胞状态在输出的激活函数前数值的裁剪范围，为None则不裁剪
- **initializer**：权重和投影矩阵的初始化值
- **num_proj**：投影矩阵输出维数，为None则进行投影
- **proj_clip**：投影数值的裁剪范围，为None则不裁剪
- **num_unit_shards**：已无效
- **num_proj_shards**：已无效
- **forget_bias**：遗忘门偏差，为了防止训练开始时LSTM遗忘过多信息，因此默认为1。但若从CudnnLSTM恢复权重需要
- **state_is_tuple**：即将无效
- **activation**：激活函数，默认为tanh
- **reuse**：是否公用同scope下变量
- **name**：层名称
-  **dtype**：层默认数据类型
- **\*\*kwargs** ：其它层公用参数

## 参考
> [LONG SHORT-TERM MEMORY](http://www.bioinf.jku.at/publications/older/2604.pdf)
> <br/>
> [Long Short-Term Memory Recurrent Neural Network Architectures
for Large Scale Acoustic Modeling](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/43905.pdf)
》
> [Step-by-step to LSTM: 解析LSTM神经网络设计原理](https://zhuanlan.zhihu.com/p/30465140)




