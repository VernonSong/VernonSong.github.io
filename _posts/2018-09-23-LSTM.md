---
layout: post
title:  LSTM原理与实现
subtitle: 
date: 2018-07-13 16:09:37 +08:00
author:     "VernonSong"
header-img: "img/post-bg-lstm.jpg"
catalog: true
tags:
    - 深度学习
    - 论文翻译
---

## 概述
RNN的最重要的特点就在于它能结合之前时间步的信息到当前时间步，以此来更好的解决时序问题。但当时间间隔很远时，普通的RNN网络便无法连接这两个时间步的信息。为解决这样的**长期依赖**问题，人们研发出了长短时记忆网络。

## 原理


但依然没有解决原

### 传统RNN缺点
传统的RNN单元设计非常简单

$$
\begin{align*}
&s_t=Wx_t+Uh_{t-1}+b_{\mathbf{h}}
\newline &h_t=tanh(s_t)
\newline &\hat{y}=softmax(Vh_t+b_{\mathbf{y}})
\end{align*}
$$


这样的结构带来了三个问题
-  前面时间步的信息在不断更新后难以有效的传递给后面，与之相对的，误差也难以有效的传递到前面，这便是RNN的长期依赖问题
- 
- 

未解决上述问题，我们需要新的机制来管理隐藏状态，尽量让需要的信息能长时间的存储。这个机制就是“门”，之所以采用门，是因为门机制能有效的控制输入和输出信息。同时让梯度恒为1，避免梯度爆炸与梯度消失。这种性质叫做常数误差流。

### 长期记忆与短期记忆
解决长期依赖的根本就是减少信息在向后传递与误差向前传递过程中发生的衰减，为了更大限度的保留信息，我们可以将每一个时间步的隐藏状态加到之前时间步上

$$
\begin{align*}
&s_t=W_\mathbf{s}x_t+U_\mathbf{s}h_{t-1}+b_\mathbf{s}
\newline &C_t=C_{t-1}+s_t
\newline & h_t=tanh(C_t)
\newline &\hat{y}=softmax(Vh_t+b_{\mathbf{y}})
\end{align*}
$$

可以把$C_{i-1}$理解为长期记忆，$s_i$理解为短期记忆，这边是长短期记忆网络（Long Short-Term Memory,LSTM）名字的由来。但LSTM远不止如此。

### 输入门
我们希望能通过$s$将信息更新进$C$中，并让$C$保持此状态，但由于所有时间步的共享权值的特性，对$s$来说更新$C$与保持$C$不变是矛盾的，因此需要通过门机制来控制每个时间步更新哪些信息到$C$中。

$$
\begin{align*}
& i_t=\sigma (W_\mathbf{i}h_{t-1}+U_\mathbf{i}x_t+b_\mathbf{i}) 
\end{align*}
$$

$i_t$为每个信息的权重，通过sigmoid函数将其归一化到0-1之间，以此来控制传入到$C$中的信息流。

### 输出门
由于$C$为长期记忆，很多信息是存储来以后使用，都作为当前时刻的输出显然不合理，所以我们同样需要一个门机制来控制输出的信息流。

$$
\begin{align*}
& o_t=\sigma (W_\mathbf{o}h_{t-1}+U_\mathbf{o}x_t+b_\mathbf{o})
\end{align*}
$$

### 遗忘门
在tanh函数激活的情况下，一直增加$C$很容易让$h$的值趋近饱和，这样继续增加$C$意义不大，因此我们还需要一个门来控制何时去”遗忘一些记忆”。

$$
\begin{align*}
f_t=\sigma (W_fh_{t-1}+U_fx_t+b_f)
\end{align*} 
### 常数误差流
由于BPTT是一个随时间累乘的过程，随着时间步变长容易出现梯度爆炸或者梯度消失现象，而梯度消失也是长期依赖问题的原因。因此我们希望梯度流恒等于1，为了获得这样的常数误差流（constant error carousel ,CEC），线性激活函数是一个非常好的选择，而tanh函数具有在输入的绝对值较小时输出近似线性的性质。这样LSTM单元为：

$$
\begin{align*}
&s_t=W_\mathbf{s}x_t+U_\mathbf{s}h_{t-1}+b_\mathbf{s}
\newline &C_t=f_t*C_{t-1}+i_t*tanh(s_t)
\newline & h_t=o_ttanh(C_t)
\newline &\hat{y}=softmax(Vh_t+b_{\mathbf{y}})
\end{align*}
$$


$C_{t}$有两种影响结果的方式，一种是转化为$h_{t}$，一种是转化为$C_{t+1}，定义

$$
\begin{align*}
& \delta _\mathbf{C}^{(t)} =\frac{\partial E_t}{\partial C_{t}}
\end{align*}
$$

根据全导数公式，

$$
\frac{\partial E}{\partial C}
$$

$$
\begin{align*}
\delta _\mathbf{C}^{(t)} &=\frac{\partial E_t}{\partial C_{t} }\frac{\partial C_{t}}{\partial C_{t-1}}+\frac{\partial E_t}{\partial h_{t-1} }\frac{\partial h_{t-1}}{\partial C_{t-1}}
\newline &=\delta _\mathbf{C}^{(t+1)} \odot f_{t+1}+V^T (\hat{y}_k-y_k) \odot (1-h_k \odot h_k) \odot o_t \odot (1-tanh^2(C_t))
\end{align*}
$$

$h_{t-1}$有三种影响结果的方式，分别是




### 猫眼


### 输出门




